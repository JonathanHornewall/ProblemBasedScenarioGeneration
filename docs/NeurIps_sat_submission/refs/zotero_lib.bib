@inproceedings{amosOptNetDifferentiableOptimization2017,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  year = {2017},
  pages = {136--145},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/amos17a.html},
  urlyear = {2025},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/jonathanhornewall/Zotero/storage/K5UF9LS5/Amos and Kolter - 2017 - OptNet Differentiable Optimization as a Layer in Neural Networks.pdf;/Users/jonathanhornewall/Zotero/storage/WKJMB2BB/Amos and Kolter - 2017 - OptNet Differentiable Optimization as a Layer in Neural Networks.pdf}
}

@article{banBigDataNewsvendor2019,
  title = {The {{Big Data Newsvendor}}: {{Practical Insights}} from {{Machine Learning}}},
  shorttitle = {The {{Big Data Newsvendor}}},
  author = {Ban, Gah-Yi and Rudin, Cynthia},
  year = {2019},
  journal = {Operations Research},
  volume = {67},
  number = {1},
  pages = {90--108},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2018.1757},
  url = {https://pubsonline.informs.org/doi/10.1287/opre.2018.1757},
  urlyear = {2025},
  abstract = {We investigate the data-driven newsvendor problem when one has ùëõn observations of ùëùp features related to the demand as well as historical demand data. Rather than a two-step process of first estimating a demand distribution then optimizing for the optimal order quantity, we propose solving the ‚Äúbig data‚Äù newsvendor problem via single-step machine-learning algorithms. Specifically, we propose algorithms based on the empirical risk minimization (ERM) principle, with and without regularization, and an algorithm based on kernel-weights optimization (KO). The ERM approaches, equivalent to high-dimensional quantile regression, can be solved by convex optimization problems and the KO approach by a sorting algorithm. We analytically justify the use of features by showing that their omission yields inconsistent decisions. We then derive finite-sample performance bounds on the out-of-sample costs of the feature-based algorithms, which quantify the effects of dimensionality and cost parameters. Our bounds, based on algorithmic stability theory, generalize known analyses for the newsvendor problem without feature information. Finally, we apply the feature-based algorithms for nurse staffing in a hospital emergency room using a data set from a large UK teaching hospital and find that (1) the best ERM and KO algorithms beat the best practice benchmark by 23\% and 24\%, respectively, in the out-of-sample cost, and (2) the best KO algorithm is faster than the best ERM algorithm by three orders of magnitude and the best practice benchmark by two orders of magnitude. The online appendices are available at https://doi.org/10.1287/opre.2018.1757.},
  keywords = {big data,machine learning,newsvendor,quantile regression,sample average approximation,statistical learning theory},
  file = {/Users/jonathanhornewall/Zotero/storage/QRSR6MZR/Ban and Rudin - 2019 - The Big Data Newsvendor Practical Insights from Machine Learning.pdf}
}

@article{caoStatisticalInferenceContextual,
  title = {Statistical {{Inference}} of {{Contextual Stochastic Optimization}} with {{Endogenous Uncertainty}}},
  author = {Cao, Junyu and Gao, Rui and Yang, Zhen},
  abstract = {This paper considers contextual stochastic optimization with endogenous uncertainty, where random outcomes depend on both contextual information and decisions. We analyze the statistical properties of solutions from two prominent approaches: predict-then-optimize (PTO), which first predicts a model between outcomes, contexts, and decisions, and then optimizes the downstream objective; and estimatethen-optimize (ETO), which directly estimates the conditional expectation of the objective and optimizes it. Unlike many existing studies that assume independent and identically distributed observations and/or decision/context-independent noise, we consider a setting where historical observations form a general time series, allowing for arbitrary dependencies between current outcomes and past realizations, contexts, and decisions. For both approaches, we establish non-asymptotic performance guarantees using two criteria, approximation error and regret, deriving slow and fast convergence rates.},
  langid = {english},
  file = {/Users/jonathanhornewall/Zotero/storage/MXTS8LM8/Cao et al. - Statistical Inference of Contextual Stochastic Optimization with Endogenous Uncertainty‚òÖ.pdf}
}

@article{dengPredictiveStochasticProgramming2022,
  title = {Predictive Stochastic Programming},
  author = {Deng, Yunxiao and Sen, Suvrajeet},
  year = {2022},
  journal = {Computational Management Science},
  volume = {19},
  number = {1},
  pages = {65--98},
  publisher = {Springer},
  url = {https://ideas.repec.org//a/spr/comgts/v19y2022i1d10.1007_s10287-021-00400-0.html},
  urlyear = {2025},
  abstract = {Several emerging applications call for a fusion of statistical learning and stochastic programming (SP). We introduce a new class of models which we refer to as Predictive Stochastic Programming (PSP). Unlike ordinary SP, PSP models work with datasets which represent random covariates, often refered to as predictors (or features) and responses (or labels) in the machine learning literature. As a result, these PSP models call for methodologies which borrow relevant concepts from both learning and optimization. We refer to such a methodology as Learning Enabled Optimization (LEO). This paper sets forth the foundation for such a framework by introducing several novel concepts such as statistical optimality, hypothesis tests for model-fidelity, generalization error of PSP, and finally, a non-parametric methodology for model selection. These new concepts, which are collectively referred to as LEO, provide a formal framework for modeling, solving, validating, and reporting solutions for PSP models. We illustrate the LEO framework by applying it to a production-marketing coordination model based on combining a pedagogical production planning model with an advertising dataset intended for sales prediction.},
  langid = {english},
  keywords = {Fusion with statistical learning,Model assessment,Stochastic programming},
  file = {/Users/jonathanhornewall/Zotero/storage/NIBTPGV6/v19y2022i1d10.1007_s10287-021-00400-0.html}
}

@article{kannanTechnicalNoteDataDriven2025,
  title = {Technical {{Note}}: {{Data-Driven Sample Average Approximation}} with {{Covariate Information}}},
  shorttitle = {Technical {{Note}}},
  author = {Kannan, Rohit and Bayraksan, G√ºzin and Luedtke, James R.},
  year = {2025},
  journal = {Operations Research},
  publisher = {INFORMS},
  issn = {0030-364X},
  doi = {10.1287/opre.2020.0533},
  url = {https://pubsonline.informs.org/doi/10.1287/opre.2020.0533},
  urlyear = {2025},
  abstract = {We study optimization for data-driven decision making when we have observations of the uncertain parameters within an optimization model together with concurrent observations of covariates. The goal is to choose a decision that minimizes the expected cost conditioned on a new covariate observation. We investigate two data-driven frameworks that integrate a machine learning prediction model within a stochastic programming sample average approximation (SAA) for approximating the solution to this problem. One SAA framework is new and uses leave-one-out residuals for scenario generation. The frameworks we investigate are flexible and accommodate parametric, nonparametric, and semiparametric regression techniques. We derive conditions on the data generation process, the prediction model, and the stochastic program under which solutions of these data-driven SAAs are consistent and asymptotically optimal, and also derive finite sample guarantees. Computational experiments validate our theoretical results, demonstrate examples where our data-driven formulations have advantages over existing approaches (even if the prediction model is misspecified), and illustrate the benefits of our data-driven formulations in the limited data regime. Funding: This research was supported by the Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program under contract number DE-AC02-06CH11357. R. Kannan also gratefully acknowledges funding through the LANL/LDRD Program and the Center for Nonlinear Studies. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0533.},
  keywords = {covariates,data-driven stochastic programming,jackknife,large deviations,Optimization,regression,sample average approximation}
}

@article{mengDifferentiableOptimizationGeneralized2021,
  title = {Differentiable {{Optimization}} of {{Generalized Nondecomposable Functions}} Using {{Linear Programs}}},
  author = {Meng, Zihang and Mukherjee, Lopamudra and Wu, Yichao and Singh, Vikas and Ravi, Sathya N.},
  year = {2021},
  journal = {Advances in neural information processing systems},
  shortjournal = {Adv Neural Inf Process Syst},
  volume = {34},
  eprint = {35387382},
  eprinttype = {pubmed},
  pages = {29129--29141},
  issn = {1049-5258},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8982830/},
  urlyear = {2025},
  abstract = {We propose a framework which makes it feasible to directly train deep neural networks with respect to popular families of task-specific non-decomposable performance measures such as AUC, multi-class AUC, F-measure and others. A feature of the optimization model that emerges from these tasks is that it involves solving a Linear Programs (LP) during training where representations learned by upstream layers characterize the constraints or the feasible set. The constraint matrix is not only large but the constraints are also modified at each iteration. We show how adopting a set of ingenious ideas proposed by Mangasarian for 1-norm SVMs ‚Äì which advocates for solving LPs with a generalized Newton method ‚Äì provides a simple and effective solution that can be run on the GPU. In particular, this strategy needs little unrolling, which makes it more efficient during the backward pass. Further, even when the constraint matrix is too large to fit on the GPU memory (say large minibatch settings), we show that running the Newton method in a lower dimensional space yields accurate gradients for training, by utilizing a statistical concept called sufficient dimension reduction. While a number of specialized algorithms have been proposed for the models that we describe here, our module turns out to be applicable without any specific adjustments or relaxations. We describe each use case, study its properties and demonstrate the efficacy of the approach over alternatives which use surrogate lower bounds and often, specialized optimization schemes. Frequently, we achieve superior computational behavior and performance improvements on common datasets used in the literature.},
  pmcid = {PMC8982830},
  file = {/Users/jonathanhornewall/Zotero/storage/NYBFFMAN/Meng et al. - 2021 - Differentiable Optimization of Generalized Nondecomposable Functions using Linear Programs.pdf}
}

@misc{sadanaSurveyContextualOptimization2024,
  title = {A {{Survey}} of {{Contextual Optimization Methods}} for {{Decision Making}} under {{Uncertainty}}},
  author = {Sadana, Utsav and Chenreddy, Abhilash and Delage, Erick and Forel, Alexandre and Frejinger, Emma and Vidal, Thibaut},
  year = {2024},
  eprint = {2306.10374},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2306.10374},
  url = {http://arxiv.org/abs/2306.10374},
  urlyear = {2025},
  abstract = {Recently there has been a surge of interest in operations research (OR) and the machine learning (ML) community in combining prediction algorithms and optimization techniques to solve decision-making problems in the face of uncertainty. This gave rise to the field of contextual optimization, under which data-driven procedures are developed to prescribe actions to the decision-maker that make the best use of the most recently updated information. A large variety of models and methods have been presented in both OR and ML literature under a variety of names, including data-driven optimization, prescriptive optimization, predictive stochastic programming, policy optimization, (smart) predict/estimate-then-optimize, decision-focused learning, (task-based) end-to-end learning/forecasting/optimization, etc. Focusing on single and two-stage stochastic programming problems, this review article identifies three main frameworks for learning policies from data and discusses their strengths and limitations. We present the existing models and methods under a uniform notation and terminology and classify them according to the three main frameworks identified. Our objective with this survey is to both strengthen the general understanding of this active field of research and stimulate further theoretical and algorithmic advancements in integrating ML and stochastic programming.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/jonathanhornewall/Zotero/storage/D626UTSM/Sadana et al. - 2024 - A Survey of Contextual Optimization Methods for Decision Making under Uncertainty.pdf;/Users/jonathanhornewall/Zotero/storage/G5S3VHLC/2306.html}
}

@inproceedings{tanLearningLinearPrograms2020,
  title = {Learning {{Linear Programs}} from {{Optimal Decisions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tan, Yingcong and Terekhov, Daria and Delong, Andrew},
  year = {2020},
  volume = {33},
  pages = {19738--19749},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/e44e875c12109e4fa3716c05008048b2-Abstract.html},
  urlyear = {2025},
  abstract = {We propose a flexible gradient-based framework for learning linear programs from optimal decisions. Linear programs are often specified by hand, using prior knowledge of relevant costs and constraints. In some applications, linear programs must instead be learned from observations of optimal decisions. Learning from optimal decisions is a particularly challenging bilevel problem, and much of the related inverse optimization literature is dedicated to special cases. We tackle the general problem, learning all parameters jointly while allowing flexible parameterizations of costs, constraints, and loss functions. We also address challenges specific to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Experiments show that our method successfully learns synthetic linear programs and minimum-cost multi-commodity flow instances for which previous methods are not directly applicable. We also provide a fast batch-mode PyTorch implementation of the homogeneous interior point algorithm, which supports gradients by implicit differentiation or backpropagation.},
  file = {/Users/jonathanhornewall/Zotero/storage/GLBQF295/Tan et al. - 2020 - Learning Linear Programs from Optimal Decisions.pdf}
}

@article{tianSolvingContextualStochastic2024,
  title = {Solving {{Contextual Stochastic Optimization Problems}} through {{Contextual Distribution Estimation}}},
  author = {Tian, Xuecheng and Jiang, Bo and Pang, King-Wah and Guo, Yu and Jin, Yong and Wang, Shuaian},
  year = {2024},
  journal = {Mathematics},
  volume = {12},
  number = {11},
  pages = {1612},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math12111612},
  url = {https://www.mdpi.com/2227-7390/12/11/1612},
  urlyear = {2025},
  abstract = {Stochastic optimization models always assume known probability distributions about uncertain parameters. However, it is unrealistic to know the true distributions. In the era of big data, with the knowledge of informative features related to uncertain parameters, this study aims to estimate the conditional distributions of uncertain parameters directly and solve the resulting contextual stochastic optimization problem by using a set of realizations drawn from estimated distributions, which is called the contextual distribution estimation method. We use an energy scheduling problem as the case study and conduct numerical experiments with real-world data. The results demonstrate that the proposed contextual distribution estimation method offers specific benefits in particular scenarios, resulting in improved decisions. This study contributes to the literature on contextual stochastic optimization problems by introducing the contextual distribution estimation method, which holds practical significance for addressing data-driven uncertain decision problems.},
  langid = {english},
  keywords = {contextual stochastic optimization,data-driven decision making,prescriptive analytics},
  file = {/Users/jonathanhornewall/Zotero/storage/JHJ3KVD2/Tian et al. - 2024 - Solving Contextual Stochastic Optimization Problems through Contextual Distribution Estimation.pdf}
}
